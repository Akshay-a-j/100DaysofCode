{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "surv_analysis(clinical_data).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qBTCEuQ_Eq7Llh3NAgkNJ6flUhezxWKh",
      "authorship_tag": "ABX9TyPiJige4jhxCKrIJkQhDG7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshay-a-j/100DaysofCode/blob/main/surv_analysis(clinical_data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNohdw01cRll",
        "outputId": "9cfcdefe-b2a6-4342-b7d5-f89b1f7c8e9d"
      },
      "source": [
        "!pip install lifelines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.7/dist-packages (0.26.0)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n",
            "Requirement already satisfied: formulaic<0.3,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.2.3)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: interface-meta>=1.2 in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.2.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->lifelines) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeEUbr6hUYuK"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import copy\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import models\n",
        "\n",
        "from lifelines.statistics import logrank_test\n",
        "from lifelines.utils import concordance_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epnmut75U3SR"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Histopathology/LUAD_labels.csv\", delimiter=\",\")\n",
        "Data_details = data.describe()\n",
        "data.loc[((data['tumor_stage'] =='stage i') | (data['tumor_stage'] =='stage ia') | (data['tumor_stage'] =='stage ib')) , 'STAGE_NUM'] = 1\n",
        "data.loc[((data['tumor_stage'] =='stage ii') | (data['tumor_stage'] =='stage iia') | (data['tumor_stage'] =='stage iib')) , 'STAGE_NUM'] = 2\n",
        "data.loc[((data['tumor_stage'] =='stage iiia') | (data['tumor_stage'] =='stage iiib')) , 'STAGE_NUM'] = 3\n",
        "data.loc[data['tumor_stage'] =='stage iv', 'STAGE_NUM'] = 4\n",
        "data.loc[data['gender'] =='male', 'GENDER'] = 1\n",
        "data.loc[data['gender'] =='female', 'GENDER'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "3plNvxEDVIJ9",
        "outputId": "31f34b81-3526-4207-b3c7-3d0e1b2c7565"
      },
      "source": [
        "data = data.dropna(subset=['submitter_id', 'age_at_diagnosis', 'survival_status', 'survival_time_months', 'STAGE_NUM','GENDER'])\n",
        "data = data.reset_index()\n",
        "data = data[['submitter_id','age_at_diagnosis', 'survival_status', 'survival_time_months', 'STAGE_NUM', 'GENDER']]\n",
        "data.info()\n",
        "data_details = data.describe()\n",
        "\n",
        "df = data[(data['STAGE_NUM'] == 1) | (data['STAGE_NUM'] == 2) |(data['STAGE_NUM'] == 3) | (data['GENDER']==1) |(data['GENDER']==0)]\n",
        "\n",
        "df.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 480 entries, 0 to 479\n",
            "Data columns (total 6 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   submitter_id          480 non-null    object \n",
            " 1   age_at_diagnosis      480 non-null    float64\n",
            " 2   survival_status       480 non-null    int64  \n",
            " 3   survival_time_months  480 non-null    float64\n",
            " 4   STAGE_NUM             480 non-null    float64\n",
            " 5   GENDER                480 non-null    float64\n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 22.6+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fad28b482d0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fad354397d0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fad31429e10>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fad28aba290>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fad28aea910>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fad201b7f90>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEICAYAAABVv+9nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZ338c9XdsISQrCBBGkdMjgRFDEKjjxjEGYMoAafcRBEkyCY4RFm0ImDGTfAcUHmhaO4IQImMaziAiKOMEjLMAoKyI5IwCCJgbBl6YBK8Pf8cU4nN5Wqrqrurq6qm+/79apX3brruadO/ercc5ejiMDMzMrnRe1OgJmZtYYDvJlZSTnAm5mVlAO8mVlJOcCbmZWUA7yZWUk5wHcJSYslHZqHPyLp/HanqRZJP5I0s93pMNvUOcCPomKQHo6I+ExEnDASaWqFiDgsIua3Ox02+kbqz11Sn6SOLePdwgHezEZMp/y5SwpJezUxfyn/UEof4CXNlfSQpNWS7pP09jx+M0lnS3pS0m8lnZwLxeZ5+o6SLpC0TNJSSZ+StFmdbf2FpJ9Ieiqv9yJJY/O0bwEvAX4gqV/SqXXW9R5Jj+R1fbRi2umSFhY+f1vSY5JWSrpR0isK03aW9ANJqyT9Mu/HTYXpIelESQ9KWiHpK5KUp71I0sdyOpZLWiBpxzxta0kLc/pW5HX35GnrfiyS9pL005y2JyVdVv9bs0408Nuw7lH6AA88BPwfYEfgDGChpN2A9wGHAfsB+wNHViw3D1gL7AW8Gvg7oN4/vIDPArsDfwXsAZwOEBHvAX4HvDUitouIs2quRJoMfA14T17XzsDEQbb7I2AS8GLgduCiwrSvAGuAXYGZ+VXpLcBrgVcCRwFvzuNn5dfBwMuA7YAv52kzSXm6R07ficBzVdb978C1wE55H740yH5YC0n6cK6srJb0gKRDJM2T9KnCPFMlLSl8XpyXuwtYk4evqFjvFyWdk4f7JJ0gaav8x79PYb5dJD0n6cWSdpJ0taQnJD2Thwcr49X2p2rlQdKNeZY7c2XqnYNtT9KnSTHiy3n+L0vqLVb4ivs22LY7TkRsUi/gDmA68BPgHwvjDwUC2BzoAf4IbFOYfgxwQ5PbOhL4VeHzYuDQBpb7BHBp4fMY4E8Dy5L+NBbWWHZs3o8dgc2A54G9C9M/BdxU+BzAQYXPlwNz8/D1wPsL0/bO69sceC/wM+CVVdLQB5yQhxcA5wET2/3db8qv/N09CuyeP/cCf0GqyHyqMN9UYEnh8+L8m9kD2AbYE3gW2D5P3wxYBhxY5bu/EPh0YV0nAf+Vh3cG/h7YFtge+Dbw/WplaJB9ugT4KKmiunVFOQ5gr8LnpraX8yeAzWuU65rb7qRX6WvwkmZIuiPXJlYA+wDjSTXjRwuzFof3BLYAlhWW+zqphjzYtnokXZprSauAhXlbzdogbRGxBniqxjY3k3SmUjPUKtIPkrzdXUjBuNZ+DnisMPwsqaY+kI5HCtMeYf0f4LeAHwOXSvq9pLMkbVFl3aeSjmx+IeleSe+tth/Wci8AWwGTJW0REYsj4qEGlz0nIh6NiOci4hHSUeLb87Q3Ac9GxM1VlrsYOLrw+V15HBHxVER8JyKejYjVwKeBNza5T8+Tfqu7R8QfIuKmWjOO0PaGtO12KnWAl7Qn8A3gZGDniBgL3EMKOMvYsNljj8Lwo6Qa/PiIGJtfO0TEKxjcZ0j/+vtGxA7Au/O2BjT66M5lxfRI2pZUA6nmXaQjkkNJtfbegcWAJ0jNTLX2s57fkwrxgJfk9T0eEc9HxBkRMRn4a1Izz4zKFUTEYxHxvojYHfhH4Ktq4uSXjYyIWAR8gHT0tzxXRHZvcPHKSsHFpCNaKATtKm4AtpV0gKReUnPo9yCVaUlfz+d3VgE3AmNV5zxXhYYrDyO0vSFtu51KHeBJTRtBCnRIOo5Ug4fUFHGKpAlKJ0I/PLBQRCwjtRufLWkHpZONfyGp3j/+9kA/sFLSBOBfK6Y/TmrLrucK4C2SDpK0JfBJan9X25P+jJ4iHX5+prAfLwDfBU7PBfzlVAnCg7gE+KCkl0raLq/7sohYK+lgSfvmH8gqUo3mz5UrkPQPhbbVZ0jfx0bzWetFxMURcRDpTzuAz5HOz2xbmG3XaotWfP42MDV/r2+nRoDP5e9y0p/BMcDVufYMMIfUbHRArgz9TR6vjVZUe3+aqTzU217lPq7J71XzplsqLqUO8BFxH3A28HNScN0X+N88+RukIH4X8CvgGlLt9IU8fQawJXAfKTBdAexWZ5NnkE7YrgR+SAquRZ8FPpabfT40SLrvJbVXXkyqzT8DLKkx+wJS08nSnNbKQ+WTSTX7x0jNKpeQ/hAacWFe5kbgt8AfgH/K03Yl5ckq4H7gp3neSq8FbpHUD1wFnBIRDze4fRshkvaW9CZJW5G+x+dIf7R3AIdLGidpV1Itf1AR8QSpPfqbwG8j4v5BZr8YeCdwLBv+EWyf07BC0jjgtCHs02CVh8rKVL3tbTB/3selwLtzM+h7SecsGtl252j3SYBOeZGuqHmk3ekYhf38HDC/3enwa9S/91cCvwBWA08DV5POsWwNXEb6o74L+CAbn2Td6MIA0hVeAfxrxfg+Kk6OAovyNrcsjNs9z9sP/IZUC153UrPaeqqk4SxSEO4nXS03uzDtRFLlaAXpyrB623t9Hv8M6ZzDQEz4bV7H2aRKzAn1tt1JL+XEbnIkbUO6/O9a0knD7wA3R0TdGkw3yc0yWwJ3k2rT15AK6ffbmjAza7lSN9HUIVKTyjOkJpr7SZcnDr6QdG6+VrbydW5TG5eOrbGee4e0N7VtT2oqWkOqqZ0NXDnC2zCzDrTJ1uDNrPPlitO7q0xaGBEnjnZ6uo0DvJlZSXXEsyXGjx8fvb29LVn3mjVrGDNmTEvW3WrdnHYY/fTfdtttT0bELqO2wWEYrMx3+/c+UpwPyWD5UK/Md0SA7+3t5dZbb23Juvv6+pg6dWpL1t1q3Zx2GP30S3qk/lydYbAy3+3f+0hxPiSD5UO9Mr8pn2Q1Myu1ujV4SReSbkNfHhH75HHjSFdk9JKukz0qIp6RJOCLwOGkZ5rMiojbW5N0q6d37g+bXmbxmUe0ICXWjLuXrmRWk9+dvzerppEa/DxgWsW4ucD1ETGJ9MTBuXn8YaTH1k4CZpMeeWtmZm1QN8BHxI2ku9CKpgMDvbbMZ/2z1KcDCyK5mfQwn3q395uZWQsM9SRrT6QHckF6xklPHp7Ahk+eW5LHLaOCpNmkWj49PT309fUNMSmD6+/vb9m6W224aZ+z79qmlxnJvOrmvDcrg2FfRRMRIanpi+kj4jxSRxBMmTIlWnW2vJvPxA837c224wIsPnbo26vUzXlvVgZDvYrm8YGml/y+PI9fyobPG5+Yx5mZ2SgbaoC/ivV9e85k/bNNrgJmKDkQWFloyjEzs1HUyGWSl5D6aRyv1BnvacCZwOWSjic9i/yoPPs1pEskF5EukzyuBWk2M7MG1A3wEXFMjUmHVJk3SB1VmJlZm/lOVjOzknKANzMrqY542JjVN5THDpjZps0B3jYw1D8SPwvFrPO4icbMrKRcg2+DRmvJc/ZdO6S7Uc3MwDV4M7PScoA3MyspB3izCpL2kHSDpPsk3SvplDx+nKTrJD2Y33fK4yXpHEmLJN0laf/27oFZ4gBvtrG1wJyImAwcCJwkaTLu6Ma6jAO8WYWIWDbQ1WRErAbuJ/Vr4I5urKv4KhqzQUjqBV4N3MIwO7pptJObnm2a76yljB2ruMOYZDj54ABvVoOk7YDvAB+IiFWpT/lkKB3dNNrJzZcuupKz727upzmSHbV0CncYkwwnH9xEY1aFpC1Iwf2iiPhuHu2ObqyrOMCbVVCqql8A3B8Rny9Mckc31lXcRGO2sTcA7wHulnRHHvcR3NGNdRkHeLMKEXEToBqT3dGNdQ0HeDOzUTKUp7XOmzZmyNsbVoCXtBhYDbwArI2IKZLGAZcBvcBi4KiIeGY42zEzs+aNxEnWgyNiv4iYkj/XutvPzMxGUSuuoql1t5+ZmY2i4bbBB3BtvuHj6/lGjlp3+22g0bv6hqsT74Zr9C7FodzR2C7V8rgT895sUzLcAH9QRCyV9GLgOkm/Lk4c7G6/Ru/qG65OvBuu0U485uy7tuk7Gtul2p2UnZj3ZpuSYTXRRMTS/L4c+B7wOmrf7WdmZqNoyAFe0hhJ2w8MA38H3EPtu/3MzGwUDef4vwf4Xn4A0+bAxRHxX5J+SfW7/czMbBQNOcBHxMPAq6qMf4oqd/uV0VBuWjAzGy3dcQbPOl61P7s5+64d9ITy4jOPaGWSzDZ5fpqkmVlJOcCbmZWUA7yZWUk5wJuZlZQDvJlZSTnAm5mVlAO8mVlJOcCbmZWUA7yZWUk5wJuZlZQDvJlZSflZNJkfHGZmZeMavJlZSTnAm5mVVCmbaIrNLfUeWWvtM5RmMT9i2KxxrsGbmZWUA7yZWUk5wJuZlVRLArykaZIekLRI0txWbMOs07jcW6cZ8QAvaTPgK8BhwGTgGEmTR3o7Zp3E5d46USuuonkdsCgiHgaQdCkwHbhvKCvzDUjWJUa03JuNhFYE+AnAo4XPS4ADKmeSNBuYnT/2S3qgBWnhn2E88GQr1t1q3Zx2aE369blBJ+85kttqUt1y30SZbzrf6uRLt+rq8j9SDv7coPkwaJlv23XwEXEecF6rtyPp1oiY0urttEI3px26P/0jrdEy73xLnA/JcPKhFSdZlwJ7FD5PzOPMyszl3jpOKwL8L4FJkl4qaUvgaOCqFmzHrJO43FvHGfEAHxFrgZOBHwP3A5dHxL0jvZ0mNNQMJOlHkmYOd2OS+iSdMITlzpX08YrRLW/CarGa6R9qPnWqES733f69jxTnQzL0fIgIv0bwBfQBJ9SZZxZwU7vTOop5cjqwsNl88qtt39dBwM+AlcDTwP8CpwH9+fUH4IXC53srvtdngK2qrPdo4BZgDbA8D78fUJ4+D/hTYb39wJ110toLBHBNxfiFwOl5uOrvDVgMHFrYdgDTK+b5zzx+Vru/l6G8Nok7WSWV8qFqZiNN0g7A1cCXgHGkq4POAL4XEdtFxHbAicDPBz5HxCvysr3A/yEFxLdVrHcO8EXgP4BdgZ68njcAWxZmPauw3u0i4lUNJv0ASX89lH0u+A0wo5DmzYGjgIeGud626YoAL+nDkpZKWp3vFDxE0jxJnyrMM1XSksLnxXm5u4A1efiKivV+UdI5ebhP0gmStpK0QtI+hfl2kfScpBdL2knS1ZKekPRMHp7YxL78FXAu8HpJ/ZJW5PHr9mdgXySdKmm5pGWSjpR0uKTfSHpa0kcK63yRpLmSHpL0lKTLJY2rk45eSSHpOEmP5n05UdJrJd2V8+DLFdv4mKRHcpoWSNqxYl0zJf1O0pOSPpqnTQM+Arwz7++dhWTsKel/8/d6raTxeZmtJS3M+7JC0i8l9TSaxzYsfwkQEZdExAsR8VxEXBsRdzWw7AzgZlJteF1zZy4nnwTeHxFXRMTqSH4VEcdGxB9HIN1nAZ8e5jp+ABwkaaf8eRpwF/DYMNfbNh0f4CXtTWrbfG1EbA+8mXRoVWv+zST9CngxcAyplnAn8P+AIwcCX77z8Cjg4uLyubB9Ny874CjgpxGxnJRn3yRdf/oS4DngyzQoIu5nwxrQ2MLkrfKf0AJSzWkvYB/SIe8VwAXAIaRa0sclvTQv90/AkcAbgd3z/F9pMEkHAJOAdwJfAD4KHAq8AjhK0hvzfLPy62DgZcB2Vfb7C8Bq0gnHT0g6lFT76yddUTKuokb2LuA40ne1JfChPH4msCPpqpSdc3491+D+dA3VebRBrmxclqffkmvIrfYb4AVJ8yUdVgh2jZgBXJRfby78Kb8e2Aq4stpCA/kA/F9go1q4pFm5QnVHflU7d/NV4C9zmRuqP+Q0Hl3YnwXDWF/DJF2YK0731JguSefksnCXpP0bWW/HB3hSW99WwGRJW0TE4ogY7JDpFNJJLoBzgA8An4+IXuAJ4PN52puAZyPi5irruJj1XzKkQHQxQEQ8FRHfiYhnI2I1qdbwxirrGIq/A/6LVLD+AMwlBb1Lgc1IfzwnRzp5dx8wECxPBD4aEUvyH9TpwDvUWNPUv0fEHyLiWlLb6CURsTwilgL/A7w6z3csKR8fjoh+4N+Ao/M2Bn7Ir82H66tJAf3fSW2Y55DaVo+v2PY3I+I3EfEccDmwXx7/PCmw75VrkbdFxKoG9qVrqLFHGxwPPBMRe5HyseW3M+V8PojUzPIN4AlJV9U7gpJ0EKnSc3lE3EZq1nhXnjweeDLSieiB+X+Wj86eI1VcDgO+D0yVtCpPWyFpfl7ksojYL7/Or5KE50i/xU9VmdaMBcAMSWNJv+vvD3N9jZpHOmKo5TBSRWwS6Wa5rzWy0o4P8BGxiBSkTweWS7pU0u41Zt8MOAIYKACPkgL5QNPMwjwdCkG7ihuAbSUdkGtN+wHfA5C0raSv56aKVcCNwNj8gx2OLUg/kAvy56ci4mnS7e4X5nHzSDV1SAV6uzy8J/C9gR8F6Q/uBdYH3sE8Xhh+rsrngW3sDjxSmPYI6Ua54jZW5oC/LSnI78P6vH+4kPYBxUPfZwvb+hbpapRLJf1e0lmStmhgX7rJukcbRMSfSH/i0yvmmQ4MBLgrgEMkqdUJi4j7I2JWREwkfYe7k47OBjMTuDYiBu64vJj1zTRPAeOLFY6I+Ot89LoaWBrpEQ9/Jp2k/WxEjM2vZq5sOx/okfTWivFrSb+vSluQKhPrRMRNwC6kI9mrc+Wj5SLiRtIJ7VqmAwty09bNpJizW731dnyAB4iIiyNioIYQpJrMGlIgGbAr6bD+VFJBAdgeWFGoOSwEdlZqM387NQJ8RLxAqlEek19X59o6wBxgb+CAiNgB+Js8vpkfXlQZt33ep2+Sak47SRpDCqADgfAJqgftR4HDCj+KsRGxda6Fj5Tfs+Ft0S8h/XAeZ/2fwm+BZaSrL/qB53LeBymAT2hkQxHxfEScERGTSYfsb6Fw8qskqj3aoDJ/1s2T83El6chm1ETEr0kVi31qzSNpG1Iz5hslPSbpMeCDwKskvQr4OfBHNv4Dg1Qpe6LweTXVy8nf56aJKyTtUWU6+Y/yDNKRY/H3+DvgJcU/R0nbkpoGH2FjC0m/81FpnmlQI+VlIx0f4CXtLelNkrYiNVs8RwrgdwCHSxonaVdSDf/P+fCwlqdZH0R/m9vDa7mY1C59LBv+EWyf07Ait+efNoTdehyYqHRDzAABu5EOvd5H2sdqj5yt9udwLvBpSXvCupPC1X5Mw3EJ8EGlG3m2Az5DOmxeC+yQ59mLVNsbQ7oCY8DjpDs7GyLpYEn75qOiVaRa1p/rLGYjQNLLJc3JlSByMD2GdPK0liNJR4yTSUe7+wF/RWrimxERK0iB96uS3iFpe6WT9vuRml/r+QHQGxGvBK5j/VFNNd8CtmbD5o5byE2e+QT+GOBM4FaqB/hzgL8lHZ13tY4P8KQCcCbpYTuPkf51/430Rd5JOuF6LekfbWtJi0mHu1sDJ5EOZQYODSeSapmHUrt5BoCIGLhed3fgR4VJXwC2yem5mdRm3qyfAPcCj0kaOKR9FliVtwvpT2R/UnDcNY/bhXT9cKUvku6avFbS6pyujR7wNkwXkvL8RlIe/oF0chdSmy2kdtbnSecKdgC2yXn/bdL3uLek2xvY1q6kJolVpOamn+Ztl0kjjzZYN0/Oxx1JzR2ttJpUdm6RtIZUlu4h1WhrmUk6n/K7iHhs4EU6CX+spM0j4izgX0hH2ANHfV/P8xSbT44A3qd0xVW/pCfzea+BK23OB15TKyH56PsTFCoYedkjgKmkOPEw6Xd9VERsVGGKiKcj4vpq09poaI/CiDZdgN/KF+mLvDoPfxs4Og+fS7pUq+1prJHu/wH2zsOnk64Z/g9gbh43l3SdcNvTWpHuA0h/WNuSjkTmk4J/1+R9G/Jsc1KgeSnpCqI7gVdUzHMScG4ePpp0ArPtaW9DPuxWGH47cHO7092ivOgF7qkx7QhSRVPAgcAvGlpnu3eqRRlVDPAvA34BLMoBZ6M77DrlRTq0vZV07e33gZ1Iba7XAw8C/0261LDtaa2S9jOAX5Nqe98i1di7Ju/blGeHky5LfIh0FRSk68Xfloe3zvm2KOfjy9qd5jblw2dzBeJO0gUQL293mluQB5eQzl89TzrKOJ50ddyJebpIV109BNwNTGlkvQO3CNsIk3Qu8O4qkxZGxImjlIZjSYfBlR6JfPehWadzOR46B3gzs5LqiGe0jB8/Pnp7e6tOW7NmDWPGjBndBI2Csu4XtG/fbrvtticjYpdR3/AQlKnMO72tNVh665b5drc9RQSvec1ropYbbrih5rRuVtb9imjfvgG3RgeU50ZeZSrzTm9rDZbeemW+Gy6TNDOzIeiIJprB3L10JbPm/rCpZRafeUT9mcw6lMu8jRTX4M3MSsoB3syspBzgzcxKygHezKykHODNzErKAd7MrKQc4M3MSsoB3syspBzgzcxKygHerIKkPSTdIOk+SfdKOiWPHyfpOkkP5ved8nhJOkfSotxv6P7t3QOzxAHebGNrgTmROv0+EDhJ0mRSj1rXR8QkUicsA33mHgZMyq/ZpH51zdquboB3bcY2NRGxLCJuz8OrSf3CTgCms77D5/mkzqbJ4xfkB/zdTOoHeLdRTrbZRhp52NhAbeZ2SdsDt0m6DphFqs2cKWkuqTbzYTaszRxAqs2MdAfQZqNCUi/wauAWoCciluVJjwE9eXgC8GhhsSV53LLCOCTNJtXw6enpoa+vr+o2e7aBOfuubSqdtdY1Gvr7+9u6/WZtSumtG+BzgV6Wh1dLKtZmpubZ5gN9pAC/rjYD3CxprKTdCj8Ms64gaTvgO8AHImKVpHXTIiIkNdUdWkScB5wHMGXKlJg6dWrV+b500ZWcfXdzD3pdfGz1dY2Gvr4+au1LJ9qU0ttUKXJtZuR0Wy2iGWXYN0lbkIL7RRHx3Tz68YHKSm6CWZ7HLwX2KCw+MY8za6uGA7xrMyOr22oRzej2fVMq3BcA90fE5wuTrgJmAmfm9ysL40+WdCmpOXKlj1itEzQUOV2bsU3MG4D3AHdLuiOP+wgpsF8u6XjgEeCoPO0a4HBgEfAscNzoJtesuroB3rUZ29RExE2Aakw+pMr8AZzU0kSZDUEjNXjXZszMulAjV9G4NmNm1oV8J6uZWUk5wJuZlZQDvJlZSTnAm5mVlAO8mVlJOcCbmZWUA7yZWUk195CXLtE794dNL7P4zCNakBIzs/ZxDd7MrKQc4M3MSsoB3syspErZBm82XJIuBN4CLI+IffK4ccBlQC+wGDgqIp7JT1z9Iukhe88Cswb6dC2jynNcc/Zdy6w65718jqs9Gul0+0JJyyXdUxjnDret7OYB0yrGzSX1QzwJuD5/hg37IZ5N6ofYrO0aaaKZhwu6bWIi4kbg6YrR00n9D5PfjyyMXxDJzcDY3AmOWVs18rjgG3NfrEXucNs2Re6HmI3T1kh6O6mP3m7rM3g46R1qG/ywCjq0trAPxWh/4d1WyJpR5n0bsCn3Q1zZ3j5n37V109tJ/SR3W5/Bw0nvsE+yDqWg5+VaVtiHYrQLYLcVsmaUeN/cD7F1laFeJvn4QBujC7ptQgb6IYaN+yGekS8yOBD3Q2wdYqgB3gXdSk3SJcDPgb0lLcl9D58J/K2kB4FD82dI/RA/TOqH+BvA+9uQZLON1G37yAV9KjBe0hLgNNzhtpVcRBxTY5L7Ie5gjTyHqtp1+2W9Tr+Rq2hc0M3MupAfVWBmVlIO8GZmJeUAb2ZWUg7wZmYl5QBvZlZSDvBmZiXlAG9mVlIO8GZmJeUenbJG7oCrVNa738ysHFyDNzMrKdfgzcxGyVBaCuZNGzPk7bkGb2ZWUi2pwUuaRuplfjPg/Ig4s84iXWko/8bgtvuy2lTKvXWPEa/BS9oM+AqpA+7JwDGSJo/0dsw6icu9daJW1OBfByyKiIcBJF1K6oz7vhZsqyv1zv1h1WdSD8a1/o7ncm8dR+kR7iO4QukdwLSIOCF/fg9wQEScXDHfuk63gb2BB2qscjzw5IgmsjOUdb+gffu2Z0Ts0obtNlTuS1zmnd7WGiy9g5b5tl1FU+x0ezCSbo2IKaOQpFFV1v2Ccu/bcJS1zDu9rTWc9LbiKhp3vG2bIpd76zitCPC/BCZJeqmkLYGjSZ1xm5WZy711nBEP8BGxFjgZ+DFwP3B5RNw7jFXWPaTtFJKOlnSLpDWSlufh9yuZJ+lPkvol9QP7SLozL9crKSRdU7G+hZJOz8NTJf15YHlJSyRdLum1FctE3n5/4XVqnna6pOfzuBWSfibp9S3Iiq75zkbKCJf7bss/p7e1hpzeET/JuqmSNAc4ldTp+I+BfmA/4EPAe4GvA0si4mNVlu0Ffgs8Dbw1In6Wxy8kXZlxuqSpwMKImChJwATSCbtTgSMi4vq8TACTImJRle2cDuwVEe+WtDlwBjAzIiaOVD6YWefwnawjQNKOwCeB90fEFRGxOpJfRcSxEfHHBld1FvDpejPldS+JiE8A5wOfazbNucZ5ETBBUluuPDGz1nKAHxmvB7YCrhzmer4K/KWkQ5tY5rvA/pKaemBFbieeATwFPNPMsmbWHTo2wEuaJukBSYskzW13euoYDzyZa8UASHo8t5n/WdLf5NEfym3gL+T3SyrW86v8/vkmtv17QMDYwrjbcxv7wOvNhWlHSVoBPAe8D3hHMd2NkLSHpBsk3SfpXkmn5PHjJF0n6cH8vlMeL0nn5O/yLkn7N7O9TYWkC/O5m3vanZZG1CoHnUrS1pJ+IenOnN4z2p2mRkjaTNKvJF3d7LIdGeC78Lbvp4DxuV17wD8AU4AXWJ/PtwMfj4jNgI8Dv8vjp+b3lwPvJNXi39rgticAAawojNs/IsYWXj8uTLs8IsYCPcA9wGsa3E7RWmBOREwGDgROyt/PXOD6iJgEXJ8/Q/oeJ+XXbOBrQ9jmpmAeMK3diWhCrXLQqf4IvCkiXkU6PzZN0oFtTlMjTiGduG9aRwZ4Crd9R2u5wy4AAAmASURBVMSfgIHbvjvVz0mFZ10aI+JG0knTor2B+Xl4PnBkHv7bwnI3kZpMPkuqmdfzduD2iFjTTIIj4klSsD1d0m5NLrssIm7Pw6tJhW8Caf+r7d90YEE+d3AzMLbZbW4KapSZjjVIOehIufz1549b5FdHX2UiaSJwBOlcW9M6NcBPAB4tfF5CZxecFaQrUr4q6R2Stpf0ItLRRzGPx0TEsjz8GKkWDbBrxSrvA7ajRm0uN3lMkHQacALwkSGm+wHSFT+nDmX5nJZe4NXALUBPjf3rqu/TmldRDjpWbu64A1gOXBcRHZ1e4Auk3+efh7Jwpwb4rhMRZwH/QvoyHs+vT5MC3c/ybFsVroNfDewwyCrPBcZVjNs9L9tPurFmX2BqRFxbMd+dFdfBf2GQ7fwHMFvSi+vv5YYkbQd8B/hARKwqTot0/W1H145sZAxWDjpNRLwQEfuR7jR+naR92p2mWiS9BVgeEbcNdR2d2qNTV972HREXkS49BNbVaq7OzUyz8k1FUyNiWW6i6MuzLiI9P3zgZOdEYH7xeeIR0UcDf8gRUbNZJyJOrzLuFqDpLmMkbUH6UV8UEd/Nox+XtFth/5bn8V35fVp9NcpBx4uIFZJuIB0ld+pJ7TcAb5N0OLA1sIOkhRHx7kZX0Kk1+LLe9n0VMDMPz2T9ZZVXATNy08uBwMpCU0fHyTdaXQDcHxHFK35KsX/WmEHKQUeStIuksXl4G9K5r1+3N1W1RcS/RcTEiOglxcCfNBPcB1bSkS/gcOA3wEPAR9udniGk/xJgGfA8qc35eGBn0tUlDwL/DYzL84p01dBDwN3AlHanv86+HURqfrkLuCO/Di/L/nVSmWl3moZSDtqdrkHS+0rSpch3kWrtn2h3mppI+1RSa0BTy/lRBWZmJdWpTTRmZjZMHXGSdfz48dHb21t12po1axgzpulzgKXjfEgGy4fbbrvtyWhTj05mnagjAnxvby+33npr1Wl9fX1MnTp1dBPUgZwPyWD5IOmR0U2NWWdzE42ZWUl1RA1+MHcvXcmsuT9sapnFZx7RotSYmXUP1+DNzErKAd7MrKQc4M3MSsoB3syspBzgzcxKygHezKykHODNzErKAd7MrKQc4M3MSqrj72S18upt8g5lgHnT/MA1s0bVrcFL2kPSDZLuk3SvpFPy+HGSrpP0YH7fKY+XpHMkLZJ0l6T9W70TZma2sUaaaNYCcyJiMnAgcJKkycBc4PqImETqxWdunv8wYFJ+zQa+NuKpNjOzuhrpxHlZRNyeh1cD9wMTgOnA/DzbfODIPDwdWBDJzcDY3AGzmZmNoqba4CX1Aq8GbgF6Yn3HyY8BPXl4AvBoYbEledwGnSxLmk2q4dPT00NfX1/VbfZsA3P2XdtMMmuuq5v19/eXbr+a/V6hnPlg1ioNB3hJ2wHfAT4QEatSh+pJRISkpjp3jYjzgPMApkyZErU6cfjSRVdy9t3NnQtefGz1dXWzMnb40exjoCGdZC1bPpi1SkOXSUraghTcL4qI7+bRjw80veT35Xn8UmCPwuIT8zgzMxtFjVxFI+AC4P6I+Hxh0lXAzDw8E7iyMH5GvprmQGBloSnHzMxGSSNtH28A3gPcLemOPO4jwJnA5ZKOBx4BjsrTrgEOBxYBzwLHjWiKzcysIXUDfETcBKjG5EOqzB/AScNMl5mZDZMfVWBmVlIO8GZmJeUAb2ZWUg7wZmYl5QBvZlZSDvBmZiXlAG9mVlIO8GZmJeUAb2ZWUg7wZmYl5QBvZlZSDvBmZiXlAG9mVlKNPA/+QknLJd1TGDdO0nWSHszvO+XxknSOpEWS7pK0fysTb2ZmtTVSg58HTKsYNxe4PiImAdfnzwCHAZPyazbwtZFJppmZNatugI+IG4GnK0ZPB+bn4fnAkYXxCyK5GRg70K2fmZmNruZ6s16vp9AN32NATx6eADxamG9JHrdRl32SZpNq+fT09NDX11d9Q9vAnH3XNpW4WuvqZv39/aXbr2a/VyhnPpi1ylAD/DoREZJiCMudB5wHMGXKlJg6dWrV+b500ZWcfXdzyVx8bPV1dbO+vj5q5VG3mjX3h00vM2/amNLlg1mrDPUqmscHml7y+/I8fimwR2G+iXmcmZmNsqEG+KuAmXl4JnBlYfyMfDXNgcDKQlOOmZmNorptH5IuAaYC4yUtAU4DzgQul3Q88AhwVJ79GuBwYBHwLHBcC9JsZmYNqBvgI+KYGpMOqTJvACcNN1FmZjZ8vpPVzKykHODNzErKAd7MrKQc4M3MSsoB3syspBzgzcxKygHezKykHODNzErKAd7MrKQc4M3MSsoB3syspBzgzcxKygHezKykWhLgJU2T9ICkRZLm1l/CzMxG2ogHeEmbAV8BDgMmA8dImjzS2zEzs8G1ogb/OmBRRDwcEX8CLgWmt2A7ZmY2iGF3ul3FBODRwuclwAGVM0maDczOH/slPVBjfeOBJ5tJgD7XzNxdo+l8KKODPzdoPuw5mmkx63StCPANiYjzgPPqzSfp1oiYMgpJ6mjOh8T5YNa4VjTRLAX2KHyemMeZmdkoakWA/yUwSdJLJW0JHA1c1YLtmJnZIEa8iSYi1ko6GfgxsBlwYUTcO4xV1m3G2UQ4HxLng1mDFBHtToOZmbWA72Q1MyspB3gzs5LqiABf79EGkraSdFmefouk3tFP5ehoIC9mSXpC0h35dUI70tlKki6UtFzSPTWmS9I5OY/ukrT/aKfRrBu0PcA3+GiD44FnImIv4D+BUt7K1MRjHi6LiP3y6/xRTeTomAdMG2T6YcCk/JoNfG0U0mTWddoe4Gns0QbTgfl5+ArgEEkaxTSOFj/mAYiIG4GnB5llOrAgkpuBsZJ2G53UmXWPTgjw1R5tMKHWPBGxFlgJ7DwqqRtdjeQFwN/npokrJO1RZXrZNZpPZpu0Tgjw1pwfAL0R8UrgOtYf2ZiZbaATAnwjjzZYN4+kzYEdgadGJXWjq25eRMRTEfHH/PF84DWjlLZO4sdhmDWgEwJ8I482uAqYmYffAfwkynmHVt28qGhrfhtw/yimr1NcBczIV9McCKyMiGXtTpRZp2nb0yQH1Hq0gaRPArdGxFXABcC3JC0inXw7un0pbp0G8+KfJb0NWEvKi1ltS3CLSLoEmAqMl7QEOA3YAiAizgWuAQ4HFgHPAse1J6Vmnc2PKjAzK6lOaKIxM7MWcIA3MyspB3gzs5JygDczKykHeDOzknKANzMrKQd4M7OS+v8Zs9CSjlK9EAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kbcU77_4M6Z",
        "outputId": "2b88773f-973d-4a96-e8e7-932205f6d900"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "submitter_id            0\n",
              "age_at_diagnosis        0\n",
              "survival_status         0\n",
              "survival_time_months    0\n",
              "STAGE_NUM               0\n",
              "GENDER                  0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO013OhzvSt6"
      },
      "source": [
        "# for i in range(len(df)):\n",
        "#   print(df.iloc[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "na-XIyXmVatf",
        "outputId": "8f36b852-0781-4396-9f9b-419cf0024f18"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>submitter_id</th>\n",
              "      <th>age_at_diagnosis</th>\n",
              "      <th>survival_status</th>\n",
              "      <th>survival_time_months</th>\n",
              "      <th>STAGE_NUM</th>\n",
              "      <th>GENDER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TCGA-05-5429</td>\n",
              "      <td>60.45</td>\n",
              "      <td>1</td>\n",
              "      <td>9.167</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCGA-44-2666</td>\n",
              "      <td>43.75</td>\n",
              "      <td>1</td>\n",
              "      <td>3.233</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TCGA-44-2668</td>\n",
              "      <td>51.66</td>\n",
              "      <td>1</td>\n",
              "      <td>25.370</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TCGA-44-3919</td>\n",
              "      <td>71.45</td>\n",
              "      <td>1</td>\n",
              "      <td>34.200</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TCGA-44-4112</td>\n",
              "      <td>60.56</td>\n",
              "      <td>1</td>\n",
              "      <td>26.930</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   submitter_id  age_at_diagnosis  ...  STAGE_NUM  GENDER\n",
              "0  TCGA-05-5429             60.45  ...        3.0     1.0\n",
              "1  TCGA-44-2666             43.75  ...        1.0     1.0\n",
              "2  TCGA-44-2668             51.66  ...        1.0     1.0\n",
              "3  TCGA-44-3919             71.45  ...        1.0     0.0\n",
              "4  TCGA-44-4112             60.56  ...        1.0     0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pyO8E5y-mv7"
      },
      "source": [
        "X = df[[\"age_at_diagnosis\", \"survival_time_months\", \"survival_status\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiqKkwuMg4rh"
      },
      "source": [
        "df = df[[\"STAGE_NUM\", \"GENDER\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04sby-Ld8i6x"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "   \n",
        "# creating one hot encoder object with categorical feature 0\n",
        "# indicating the first column\n",
        "columnTransformer = ColumnTransformer([('encoder',\n",
        "                                        OneHotEncoder(),\n",
        "                                        [0])],\n",
        "                                      remainder='passthrough')\n",
        "  \n",
        "data = pd.DataFrame(np.array(columnTransformer.fit_transform(df), dtype = np.str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "cf23ttc593Aj",
        "outputId": "5f7ec9bf-cb04-4a90-f206-38c5a6ba7a19"
      },
      "source": [
        "df = pd.concat([X, data], axis=1)\n",
        "df.columns = [\"age_at_diagnosis\", \"survival_time_months\", \"survival_status\",\"Stage1\", \"Stage2\", \"Stage3\", \"Male\", \"Female\" ]\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_at_diagnosis</th>\n",
              "      <th>survival_time_months</th>\n",
              "      <th>survival_status</th>\n",
              "      <th>Stage1</th>\n",
              "      <th>Stage2</th>\n",
              "      <th>Stage3</th>\n",
              "      <th>Male</th>\n",
              "      <th>Female</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60.45</td>\n",
              "      <td>9.1670</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43.75</td>\n",
              "      <td>3.2330</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51.66</td>\n",
              "      <td>25.3700</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71.45</td>\n",
              "      <td>34.2000</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60.56</td>\n",
              "      <td>26.9300</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>56.52</td>\n",
              "      <td>0.4333</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>67.78</td>\n",
              "      <td>20.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>49.66</td>\n",
              "      <td>20.5700</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>74.58</td>\n",
              "      <td>59.9300</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>68.23</td>\n",
              "      <td>17.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>480 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age_at_diagnosis  survival_time_months  survival_status  ... Stage3 Male Female\n",
              "0               60.45                9.1670                1  ...    1.0  0.0    1.0\n",
              "1               43.75                3.2330                1  ...    0.0  0.0    1.0\n",
              "2               51.66               25.3700                1  ...    0.0  0.0    1.0\n",
              "3               71.45               34.2000                1  ...    0.0  0.0    0.0\n",
              "4               60.56               26.9300                1  ...    0.0  0.0    0.0\n",
              "..                ...                   ...              ...  ...    ...  ...    ...\n",
              "475             56.52                0.4333                0  ...    0.0  0.0    0.0\n",
              "476             67.78               20.1000                0  ...    0.0  0.0    1.0\n",
              "477             49.66               20.5700                0  ...    1.0  0.0    1.0\n",
              "478             74.58               59.9300                1  ...    0.0  0.0    0.0\n",
              "479             68.23               17.1000                0  ...    0.0  0.0    0.0\n",
              "\n",
              "[480 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "AbXjwlPPhYTz",
        "outputId": "68c7ebf0-6cf0-494c-ba00-cd0e8dc66043"
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(df.corr(), annot=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tEC1P4Su_u7F",
        "outputId": "0b210afc-8d23-4dfc-b362-9036860aa72e"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age_at_diagnosis</th>\n",
              "      <th>survival_time_months</th>\n",
              "      <th>survival_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60.45</td>\n",
              "      <td>9.1670</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>43.75</td>\n",
              "      <td>3.2330</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51.66</td>\n",
              "      <td>25.3700</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71.45</td>\n",
              "      <td>34.2000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60.56</td>\n",
              "      <td>26.9300</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>56.52</td>\n",
              "      <td>0.4333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>67.78</td>\n",
              "      <td>20.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>49.66</td>\n",
              "      <td>20.5700</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>74.58</td>\n",
              "      <td>59.9300</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>68.23</td>\n",
              "      <td>17.1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>480 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age_at_diagnosis  survival_time_months  survival_status\n",
              "0               60.45                9.1670                1\n",
              "1               43.75                3.2330                1\n",
              "2               51.66               25.3700                1\n",
              "3               71.45               34.2000                1\n",
              "4               60.56               26.9300                1\n",
              "..                ...                   ...              ...\n",
              "475             56.52                0.4333                0\n",
              "476             67.78               20.1000                0\n",
              "477             49.66               20.5700                0\n",
              "478             74.58               59.9300                1\n",
              "479             68.23               17.1000                0\n",
              "\n",
              "[480 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB-Y1l91YhL6",
        "outputId": "0ddfa25a-9cce-41d3-b277-927f55de9442"
      },
      "source": [
        "X = np.array(df[[\"age_at_diagnosis\",\"survival_time_months\",\"Stage1\", \"Stage2\", \"Stage3\", \"Male\", \"Female\"]], dtype= np.float32())#, ]])\n",
        "Y = np.array(df[[\"survival_status\"]])\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "\n",
        "inputs = torch.from_numpy(X)\n",
        "labels = torch.from_numpy(Y)\n",
        "print(inputs.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(480, 7) (480, 1)\n",
            "torch.Size([480, 7]) torch.Size([480, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGteUcG-aIbH",
        "outputId": "1dfd7ead-d186-4f15-8322-f02e74358c7c"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, test_size= 0.2, random_state=0, shuffle = True)\n",
        "print(X_train.shape, X_test.shape,Y_train.shape, Y_test.shape )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([384, 7]) torch.Size([96, 7]) torch.Size([384, 1]) torch.Size([96, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8wRxqGvwF80"
      },
      "source": [
        "# from sklearn import preprocessing\n",
        "# def scalar_op(x):\n",
        "#   scaler = preprocessing.StandardScaler()\n",
        "#   arr_norm = scaler.fit_transform(x) \n",
        "#   m = x.mean(0, keepdim=True)\n",
        "#   s = x.std(0, unbiased=False, keepdim=True)\n",
        "#   x -= m\n",
        "#   x /= s\n",
        "\n",
        "#   torch.allclose(x, torch.from_numpy(arr_norm))\n",
        "#   return x\n",
        "\n",
        "def normz(x):\n",
        "  xmax = torch.max(x)\n",
        "  xmin = torch.min(x)\n",
        "  x = (x - xmin)/(xmax - xmin)\n",
        "  return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMPInUrOiDrK"
      },
      "source": [
        "# X_train = normz(X_train)\n",
        "# X_test = normz(X_test)\n",
        "# X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmvJJX1ZygB"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_ds = TensorDataset(X_train, Y_train)\n",
        "test_ds = TensorDataset(X_test, Y_test)\n",
        "\n",
        "batch_size = 32\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIqLvNWdViKP"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "class Surv_nw(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Surv_nw, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
        "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.tanh(self.hidden(x)) \n",
        "        #x = F.dropout(x)     # activation function for hidden layer\n",
        "        x = self.predict(x)             # linear output\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQikl_VHWlPV"
      },
      "source": [
        "net = Surv_nw(n_feature=1, n_hidden=16, n_output=1)     # define the network\n",
        "# print(net)  # net architecture\n",
        "opt = torch.optim.SGD(net.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMcZHjo2W2H0"
      },
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def accuracy_cox(hazards, labels):\n",
        "    correct = 0\n",
        "    # This accuracy is based on estimated survival events against true survival events\n",
        "    hazardsdata = hazards.cpu().numpy().reshape(-1)\n",
        "    median = np.median(hazardsdata)\n",
        "    hazards_dichotomize = np.zeros([len(hazardsdata)], dtype=int)\n",
        "    hazards_dichotomize[hazardsdata > median] = 1\n",
        "    hazards_dichotomize[hazardsdata <= median] = 0\n",
        "    labels = labels.data.cpu().numpy().reshape(-1)\n",
        "    correct = np.sum(hazards_dichotomize == labels)\n",
        "    return (correct/len(labels))\n",
        "\n",
        "def cox_log_rank(hazards, labels, survtime_all):\n",
        "    hazardsdata = hazards.cpu().numpy().reshape(-1)\n",
        "    median = np.median(hazardsdata)\n",
        "    hazards_dichotomize = np.zeros([len(hazardsdata)], dtype=int)\n",
        "    hazards_dichotomize[hazardsdata > median] = 1\n",
        "    survtime_all = survtime_all.data.cpu().numpy().reshape(-1)\n",
        "    idx = hazards_dichotomize == 0\n",
        "    labels = labels.data.cpu().numpy()\n",
        "    T1 = survtime_all[idx]\n",
        "    T2 = survtime_all[~idx]\n",
        "    E1 = labels[idx]\n",
        "    E2 = labels[~idx]\n",
        "    results = logrank_test(T1, T2, event_observed_A=E1, event_observed_B=E2)\n",
        "    pvalue_pred = results.p_value\n",
        "    return (pvalue_pred)\n",
        "\n",
        "\n",
        "def CIndex(hazards, labels, survtime_all):\n",
        "    labels = labels.data.cpu().numpy()\n",
        "    concord = 0.\n",
        "    total = 0.\n",
        "    N_test = labels.shape[0]\n",
        "    labels = np.asarray(labels, dtype=bool)\n",
        "    for i in range(N_test):\n",
        "        if labels[i] == 1:\n",
        "            for j in range(N_test):\n",
        "                if survtime_all[j] > survtime_all[i]:\n",
        "                    total = total + 1\n",
        "                    if hazards[j] < hazards[i]:\n",
        "                        concord = concord + 1\n",
        "                    elif hazards[j] < hazards[i]:\n",
        "                        concord = concord + 0.5\n",
        "\n",
        "    return (concord / total)\n",
        "\n",
        "\n",
        "def CIndex_lifeline(hazards, labels, survtime_all):\n",
        "     survtime_all = survtime_all.cpu().numpy().reshape(-1)\n",
        "     labels = labels.data.cpu().numpy()# X = iter(trainloader).next()\n",
        "     hazards = hazards.cpu().numpy().reshape(-1)\n",
        "     return(concordance_index(survtime_all, -hazards, labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqpUwgOHe5Q3"
      },
      "source": [
        "def test(model, dataloader, verbose):\n",
        "  \n",
        "  model.eval()\n",
        "  lbl_pred_all = None\n",
        "  lbl_all = None\n",
        "  survtime_all = None\n",
        "  loss_nn_sum = 0\n",
        "  iter = 0\n",
        "  gc.collect()\n",
        "  for i_batch, sample in enumerate(dataloader, 0):\n",
        "      # Generate predictions\n",
        "      X, Y = sample\n",
        "      labels = Y.cuda()\n",
        "      survtime = X[:, 1]\n",
        "      age, gender, survtime, stg = X[:, 0], X[:, 1], X[:,2], X[:,3]\n",
        "      age = torch.unsqueeze(age, 1).cuda()\n",
        "     \n",
        "      #inputs = torch.cat([age[None, :], gender[None, :], stg[None, :]], dim=0)\n",
        "      #inputs = torch.transpose(torch.cat([age[None, :], gender[None, :], stg[None, :]], dim=0), 1, 0)\n",
        "      #inputs = torch.cat((torch.unsqueeze(X[:,0], 1), X[:, 2:]), dim=1)\n",
        "      pred = model(age.float().cuda())\n",
        "\n",
        "      if iter == 0:\n",
        "          lbl_pred_all = pred\n",
        "          survtime_all = survtime\n",
        "          lbl_all = labels\n",
        "          \n",
        "  \n",
        "      else:\n",
        "          lbl_pred_all = torch.cat([lbl_pred_all, pred])\n",
        "          lbl_all = torch.cat([lbl_all, labels])\n",
        "          survtime_all = torch.cat([survtime_all, survtime])\n",
        "  \n",
        "  \n",
        "      current_batch_len = len(survtime)\n",
        "      \n",
        "      R_matrix_train = np.zeros([current_batch_len, current_batch_len], dtype=float)\n",
        "      \n",
        "\n",
        "      for i in range(current_batch_len):\n",
        "          for j in range(current_batch_len):\n",
        "              R_matrix_train[i, j] = survtime[j] >= survtime[i]\n",
        "  \n",
        "      train_R = torch.FloatTensor(R_matrix_train)\n",
        "      train_R = train_R.cuda()\n",
        "      train_ystatus = labels\n",
        "  \n",
        "      theta = pred.reshape(-1)\n",
        "      exp_theta = torch.exp(theta)\n",
        "      loss_nn = -torch.mean( (theta - torch.log(torch.sum( exp_theta*train_R ,dim=1))) * train_ystatus.float() )\n",
        "      \n",
        "      l1_reg = None\n",
        "      for W in model.parameters():\n",
        "          if l1_reg is None:\n",
        "              l1_reg = torch.abs(W).sum()\n",
        "          else:\n",
        "              l1_reg = l1_reg + torch.abs(W).sum()  # torch.abs(W).sum() is equivalent to W.norm(1)\n",
        "\n",
        "      loss = loss_nn + 1e-5 * l1_reg\n",
        "      loss_nn_sum = loss_nn_sum + loss_nn.data.item()\n",
        "      iter += 1\n",
        "\n",
        "  acc_test = accuracy_cox(lbl_pred_all.data, lbl_all)\n",
        "  pvalue_pred = cox_log_rank(lbl_pred_all.data, lbl_all, survtime_all)\n",
        "  c_index = CIndex_lifeline(lbl_pred_all.data, lbl_all, survtime_all)\n",
        "  if verbose > 0:\n",
        "      print('\\n[Testing]\\t loss (nn):{:.4f}'.format(loss_nn_sum),\n",
        "                    'c_index: {:.4f}, p-value: {:.3e}'.format(c_index, pvalue_pred))  \n",
        "\n",
        "\n",
        "  return(loss.data.item(), loss_nn_sum, acc_test, pvalue_pred, c_index, lbl_pred_all.data.cpu().numpy().reshape(-1), survtime_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAzX2QpfW20l"
      },
      "source": [
        "def fit(num_epochs, model, lr, measure=1, verbose=1):\n",
        "    c_index_list = {}\n",
        "    c_index_all = {}\n",
        "    loss_nn_all = {}\n",
        "    pvalue_all = {}\n",
        "    acc_train_all = {}\n",
        "\n",
        "    c_index_list['train'] = []\n",
        "    c_index_list['test'] = []\n",
        "\n",
        "    loss_nn_all['train'] = []\n",
        "    loss_nn_all['test'] = []\n",
        "\n",
        "    pvalue_all['train'] = []\n",
        "    pvalue_all['test'] = []\n",
        "\n",
        "    c_index_all['train'] = []\n",
        "    c_index_all['test'] = []\n",
        "\n",
        "    acc_train_all['train'] = []\n",
        "    acc_train_all['test'] = []\n",
        "\n",
        "    c_index_best = 0\n",
        "    test_ci_loss = []\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model = model.cuda()\n",
        "        model.train()\n",
        "        lbl_pred_all = None\n",
        "        lbl_all = None\n",
        "        survtime_all = None\n",
        "        loss_nn_sum = 0\n",
        "        iter = 0\n",
        "        gc.collect()\n",
        "        \n",
        "        for i_batch, sample in enumerate(train_dl, 0):\n",
        "            \n",
        "            # Generate predictions\n",
        "            X, Y = sample\n",
        "            #inputs = X.cuda()\n",
        "            labels = Y.cuda()\n",
        "            survtime = X[:, 1]\n",
        "            age, gender, survtime, stg = X[:, 0], X[:, 1], X[:,2], X[:,3]\n",
        "            age = torch.unsqueeze(age, 1).cuda()\n",
        "            #inputs = torch.cat([age[None, :], gender[None, :], stg[None, :]], dim=1)\n",
        "            #inputs = torch.transpose(torch.cat([age[None, :], gender[None, :], stg[None, :]], dim=0), 1, 0)\n",
        "            #inputs = torch.cat((torch.unsqueeze(X[:,0], 1), X[:, 2:]), dim=1)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            pred = model(age.float().cuda())\n",
        "\n",
        "            if iter == 0:\n",
        "                lbl_pred_all = pred\n",
        "                survtime_all = survtime\n",
        "                lbl_all = labels\n",
        "                \n",
        "        \n",
        "            else:\n",
        "                lbl_pred_all = torch.cat([lbl_pred_all, pred])\n",
        "                lbl_all = torch.cat([lbl_all, labels  ])\n",
        "                survtime_all = torch.cat([survtime_all, survtime])\n",
        "        \n",
        "        \n",
        "            current_batch_len = len(survtime)\n",
        "            R_matrix_train = np.zeros([current_batch_len, current_batch_len], dtype=float)\n",
        "            \n",
        "            for i in range(current_batch_len):\n",
        "                for j in range(current_batch_len):\n",
        "                    R_matrix_train[i, j] = survtime[j] >= survtime[i]\n",
        "        \n",
        "            train_R = torch.FloatTensor(R_matrix_train)\n",
        "            train_R = train_R.cuda()\n",
        "            train_ystatus = labels\n",
        "        \n",
        "            theta = pred.reshape(-1)\n",
        "            exp_theta = torch.exp(theta)\n",
        "            loss_nn = -torch.mean( (theta - torch.log(torch.sum( exp_theta*train_R ,dim=1))) * train_ystatus.float() )\n",
        "           \n",
        "            l1_reg = None\n",
        "            for W in model.parameters():\n",
        "                if l1_reg is None:\n",
        "                    l1_reg = torch.abs(W).sum()\n",
        "                else:\n",
        "                    l1_reg = l1_reg + torch.abs(W).sum()  # torch.abs(W).sum() is equivalent to W.norm(1)\n",
        "\n",
        "            loss = loss_nn + 1e-5 * l1_reg\n",
        "            loss_nn_sum = loss_nn_sum + loss_nn.data.item()\n",
        "    # ===================backward====================\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "            iter += 1\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "\n",
        "        print('epoch:{:.1f}'.format(epoch))\n",
        "        if measure or epoch == (epoch - 1):\n",
        "          acc_train = accuracy_cox(lbl_pred_all.data, lbl_all)\n",
        "          pvalue_pred = cox_log_rank(lbl_pred_all.data, lbl_all, survtime_all)\n",
        "          c_index = CIndex_lifeline(lbl_pred_all.data, lbl_all, survtime_all)\n",
        "                \n",
        "          c_index_list['train'].append(c_index)\n",
        "          \n",
        "          if c_index > c_index_best:\n",
        "              c_index_best = c_index\n",
        "          if verbose > 0:\n",
        "              print(\"epcoh:\", epoch )\n",
        "              print('\\n[Training]\\t loss (nn):{:.4f}'.format(loss_nn_sum),\n",
        "                    'c_index: {:.4f}, p-value: {:.3e}'.format(c_index, pvalue_pred))\n",
        "          \n",
        "          pvalue_all['train'].append(pvalue_pred)\n",
        "          c_index_all['train'].append(c_index)\n",
        "          loss_nn_all['train'].append(loss_nn_sum)\n",
        "          acc_train_all['train'].append(acc_train)\n",
        "\n",
        "          test_loss, loss_nn_sum_test, acc_test, pvalue_pred_test, c_index_pred, lbl_pred_all, surv_time_all= test(model, test_dl, verbose)\n",
        "              \n",
        "          pvalue_all['test'].append(pvalue_pred_test)\n",
        "          loss_nn_all['test'].append(loss_nn_sum_test)\n",
        "          acc_train_all['test'].append(acc_test)  \n",
        "          c_index_list['test'].append(c_index_pred)\n",
        "\n",
        "\n",
        "    return(loss_nn_all, pvalue_all, c_index_all, c_index_list, acc_train_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SF-ZFUugQ2x",
        "outputId": "4049b8d5-98c7-47a4-d9ef-8ec5a162300b"
      },
      "source": [
        "result = fit(100, net, 0.0001, measure=1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:0.0\n",
            "epcoh: 0\n",
            "\n",
            "[Training]\t loss (nn):14.2662 c_index: 0.5095, p-value: 6.910e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:1.0\n",
            "epcoh: 1\n",
            "\n",
            "[Training]\t loss (nn):14.2325 c_index: 0.4879, p-value: 5.713e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:2.0\n",
            "epcoh: 2\n",
            "\n",
            "[Training]\t loss (nn):14.2202 c_index: 0.4989, p-value: 5.411e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:3.0\n",
            "epcoh: 3\n",
            "\n",
            "[Training]\t loss (nn):14.2129 c_index: 0.5523, p-value: 5.063e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:4.0\n",
            "epcoh: 4\n",
            "\n",
            "[Training]\t loss (nn):14.2322 c_index: 0.4466, p-value: 2.581e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:5.0\n",
            "epcoh: 5\n",
            "\n",
            "[Training]\t loss (nn):14.2180 c_index: 0.5013, p-value: 4.049e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:6.0\n",
            "epcoh: 6\n",
            "\n",
            "[Training]\t loss (nn):14.2616 c_index: 0.4936, p-value: 6.577e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:7.0\n",
            "epcoh: 7\n",
            "\n",
            "[Training]\t loss (nn):14.2504 c_index: 0.4627, p-value: 2.712e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:8.0\n",
            "epcoh: 8\n",
            "\n",
            "[Training]\t loss (nn):14.2336 c_index: 0.5051, p-value: 3.356e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:9.0\n",
            "epcoh: 9\n",
            "\n",
            "[Training]\t loss (nn):14.2463 c_index: 0.4979, p-value: 4.916e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:10.0\n",
            "epcoh: 10\n",
            "\n",
            "[Training]\t loss (nn):14.2153 c_index: 0.4652, p-value: 1.183e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:11.0\n",
            "epcoh: 11\n",
            "\n",
            "[Training]\t loss (nn):14.2407 c_index: 0.5097, p-value: 6.521e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:12.0\n",
            "epcoh: 12\n",
            "\n",
            "[Training]\t loss (nn):14.2433 c_index: 0.5137, p-value: 7.369e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:13.0\n",
            "epcoh: 13\n",
            "\n",
            "[Training]\t loss (nn):14.2315 c_index: 0.4681, p-value: 3.794e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:14.0\n",
            "epcoh: 14\n",
            "\n",
            "[Training]\t loss (nn):14.2853 c_index: 0.5005, p-value: 6.577e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:15.0\n",
            "epcoh: 15\n",
            "\n",
            "[Training]\t loss (nn):14.2211 c_index: 0.4968, p-value: 3.356e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:16.0\n",
            "epcoh: 16\n",
            "\n",
            "[Training]\t loss (nn):14.2244 c_index: 0.5275, p-value: 2.813e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:17.0\n",
            "epcoh: 17\n",
            "\n",
            "[Training]\t loss (nn):14.2433 c_index: 0.5141, p-value: 8.606e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:18.0\n",
            "epcoh: 18\n",
            "\n",
            "[Training]\t loss (nn):14.2270 c_index: 0.4978, p-value: 7.541e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:19.0\n",
            "epcoh: 19\n",
            "\n",
            "[Training]\t loss (nn):14.2227 c_index: 0.4914, p-value: 7.477e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:20.0\n",
            "epcoh: 20\n",
            "\n",
            "[Training]\t loss (nn):14.2373 c_index: 0.4799, p-value: 2.981e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:21.0\n",
            "epcoh: 21\n",
            "\n",
            "[Training]\t loss (nn):14.2316 c_index: 0.4636, p-value: 3.754e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:22.0\n",
            "epcoh: 22\n",
            "\n",
            "[Training]\t loss (nn):14.2521 c_index: 0.5629, p-value: 2.101e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:23.0\n",
            "epcoh: 23\n",
            "\n",
            "[Training]\t loss (nn):14.2318 c_index: 0.4892, p-value: 9.386e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:24.0\n",
            "epcoh: 24\n",
            "\n",
            "[Training]\t loss (nn):14.2613 c_index: 0.5236, p-value: 6.577e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:25.0\n",
            "epcoh: 25\n",
            "\n",
            "[Training]\t loss (nn):14.2537 c_index: 0.4785, p-value: 2.158e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:26.0\n",
            "epcoh: 26\n",
            "\n",
            "[Training]\t loss (nn):14.2226 c_index: 0.4300, p-value: 1.530e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:27.0\n",
            "epcoh: 27\n",
            "\n",
            "[Training]\t loss (nn):14.2612 c_index: 0.5206, p-value: 4.489e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:28.0\n",
            "epcoh: 28\n",
            "\n",
            "[Training]\t loss (nn):14.2317 c_index: 0.4868, p-value: 5.411e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:29.0\n",
            "epcoh: 29\n",
            "\n",
            "[Training]\t loss (nn):14.2424 c_index: 0.4983, p-value: 6.684e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:30.0\n",
            "epcoh: 30\n",
            "\n",
            "[Training]\t loss (nn):14.2240 c_index: 0.5196, p-value: 5.210e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:31.0\n",
            "epcoh: 31\n",
            "\n",
            "[Training]\t loss (nn):14.2245 c_index: 0.5312, p-value: 4.211e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:32.0\n",
            "epcoh: 32\n",
            "\n",
            "[Training]\t loss (nn):14.2167 c_index: 0.4646, p-value: 4.540e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:33.0\n",
            "epcoh: 33\n",
            "\n",
            "[Training]\t loss (nn):14.2206 c_index: 0.4965, p-value: 8.188e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:34.0\n",
            "epcoh: 34\n",
            "\n",
            "[Training]\t loss (nn):14.2503 c_index: 0.4716, p-value: 1.597e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:35.0\n",
            "epcoh: 35\n",
            "\n",
            "[Training]\t loss (nn):14.2389 c_index: 0.5031, p-value: 5.562e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:36.0\n",
            "epcoh: 36\n",
            "\n",
            "[Training]\t loss (nn):14.2113 c_index: 0.5061, p-value: 3.754e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:37.0\n",
            "epcoh: 37\n",
            "\n",
            "[Training]\t loss (nn):14.2188 c_index: 0.5028, p-value: 6.358e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:38.0\n",
            "epcoh: 38\n",
            "\n",
            "[Training]\t loss (nn):14.2391 c_index: 0.5003, p-value: 9.697e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:39.0\n",
            "epcoh: 39\n",
            "\n",
            "[Training]\t loss (nn):14.2349 c_index: 0.4822, p-value: 5.063e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:40.0\n",
            "epcoh: 40\n",
            "\n",
            "[Training]\t loss (nn):14.2288 c_index: 0.4414, p-value: 5.034e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:41.0\n",
            "epcoh: 41\n",
            "\n",
            "[Training]\t loss (nn):14.2242 c_index: 0.4485, p-value: 2.485e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:42.0\n",
            "epcoh: 42\n",
            "\n",
            "[Training]\t loss (nn):14.2273 c_index: 0.5115, p-value: 6.222e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:43.0\n",
            "epcoh: 43\n",
            "\n",
            "[Training]\t loss (nn):14.2615 c_index: 0.5255, p-value: 5.210e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:44.0\n",
            "epcoh: 44\n",
            "\n",
            "[Training]\t loss (nn):14.2330 c_index: 0.5343, p-value: 1.186e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:45.0\n",
            "epcoh: 45\n",
            "\n",
            "[Training]\t loss (nn):14.2232 c_index: 0.4979, p-value: 9.697e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:46.0\n",
            "epcoh: 46\n",
            "\n",
            "[Training]\t loss (nn):14.2172 c_index: 0.4636, p-value: 2.272e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:47.0\n",
            "epcoh: 47\n",
            "\n",
            "[Training]\t loss (nn):14.2260 c_index: 0.4950, p-value: 6.300e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:48.0\n",
            "epcoh: 48\n",
            "\n",
            "[Training]\t loss (nn):14.2282 c_index: 0.4897, p-value: 9.207e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:49.0\n",
            "epcoh: 49\n",
            "\n",
            "[Training]\t loss (nn):14.2375 c_index: 0.5106, p-value: 8.188e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:50.0\n",
            "epcoh: 50\n",
            "\n",
            "[Training]\t loss (nn):14.2474 c_index: 0.5100, p-value: 9.632e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:51.0\n",
            "epcoh: 51\n",
            "\n",
            "[Training]\t loss (nn):14.2687 c_index: 0.5490, p-value: 1.475e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:52.0\n",
            "epcoh: 52\n",
            "\n",
            "[Training]\t loss (nn):14.2294 c_index: 0.4933, p-value: 5.411e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:53.0\n",
            "epcoh: 53\n",
            "\n",
            "[Training]\t loss (nn):14.2106 c_index: 0.5043, p-value: 7.369e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:54.0\n",
            "epcoh: 54\n",
            "\n",
            "[Training]\t loss (nn):14.2254 c_index: 0.5162, p-value: 6.081e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:55.0\n",
            "epcoh: 55\n",
            "\n",
            "[Training]\t loss (nn):14.2331 c_index: 0.5389, p-value: 3.590e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:56.0\n",
            "epcoh: 56\n",
            "\n",
            "[Training]\t loss (nn):14.2258 c_index: 0.5007, p-value: 2.712e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:57.0\n",
            "epcoh: 57\n",
            "\n",
            "[Training]\t loss (nn):14.2595 c_index: 0.5581, p-value: 2.631e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:58.0\n",
            "epcoh: 58\n",
            "\n",
            "[Training]\t loss (nn):14.2306 c_index: 0.5346, p-value: 2.272e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:59.0\n",
            "epcoh: 59\n",
            "\n",
            "[Training]\t loss (nn):14.2325 c_index: 0.4869, p-value: 5.411e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:60.0\n",
            "epcoh: 60\n",
            "\n",
            "[Training]\t loss (nn):14.2246 c_index: 0.5543, p-value: 6.744e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:61.0\n",
            "epcoh: 61\n",
            "\n",
            "[Training]\t loss (nn):14.2421 c_index: 0.5283, p-value: 3.318e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:62.0\n",
            "epcoh: 62\n",
            "\n",
            "[Training]\t loss (nn):14.2254 c_index: 0.4903, p-value: 5.981e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:63.0\n",
            "epcoh: 63\n",
            "\n",
            "[Training]\t loss (nn):14.2408 c_index: 0.4969, p-value: 8.011e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:64.0\n",
            "epcoh: 64\n",
            "\n",
            "[Training]\t loss (nn):14.2332 c_index: 0.4732, p-value: 2.363e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:65.0\n",
            "epcoh: 65\n",
            "\n",
            "[Training]\t loss (nn):14.2194 c_index: 0.5086, p-value: 3.632e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:66.0\n",
            "epcoh: 66\n",
            "\n",
            "[Training]\t loss (nn):14.2165 c_index: 0.4560, p-value: 5.505e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:67.0\n",
            "epcoh: 67\n",
            "\n",
            "[Training]\t loss (nn):14.2321 c_index: 0.5031, p-value: 5.926e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:68.0\n",
            "epcoh: 68\n",
            "\n",
            "[Training]\t loss (nn):14.2408 c_index: 0.4993, p-value: 7.369e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:69.0\n",
            "epcoh: 69\n",
            "\n",
            "[Training]\t loss (nn):14.2526 c_index: 0.5195, p-value: 5.063e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:70.0\n",
            "epcoh: 70\n",
            "\n",
            "[Training]\t loss (nn):14.2384 c_index: 0.4799, p-value: 3.590e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:71.0\n",
            "epcoh: 71\n",
            "\n",
            "[Training]\t loss (nn):14.2325 c_index: 0.4711, p-value: 7.308e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:72.0\n",
            "epcoh: 72\n",
            "\n",
            "[Training]\t loss (nn):14.2569 c_index: 0.4770, p-value: 9.697e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:73.0\n",
            "epcoh: 73\n",
            "\n",
            "[Training]\t loss (nn):14.2602 c_index: 0.4735, p-value: 3.165e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:74.0\n",
            "epcoh: 74\n",
            "\n",
            "[Training]\t loss (nn):14.2376 c_index: 0.5277, p-value: 7.541e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:75.0\n",
            "epcoh: 75\n",
            "\n",
            "[Training]\t loss (nn):14.2441 c_index: 0.5167, p-value: 6.910e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:76.0\n",
            "epcoh: 76\n",
            "\n",
            "[Training]\t loss (nn):14.2374 c_index: 0.4638, p-value: 1.965e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:77.0\n",
            "epcoh: 77\n",
            "\n",
            "[Training]\t loss (nn):14.2473 c_index: 0.5125, p-value: 8.541e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:78.0\n",
            "epcoh: 78\n",
            "\n",
            "[Training]\t loss (nn):14.2289 c_index: 0.4816, p-value: 2.454e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:79.0\n",
            "epcoh: 79\n",
            "\n",
            "[Training]\t loss (nn):14.2695 c_index: 0.4706, p-value: 2.180e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:80.0\n",
            "epcoh: 80\n",
            "\n",
            "[Training]\t loss (nn):14.2505 c_index: 0.5514, p-value: 8.954e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:81.0\n",
            "epcoh: 81\n",
            "\n",
            "[Training]\t loss (nn):14.2195 c_index: 0.5003, p-value: 6.910e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:82.0\n",
            "epcoh: 82\n",
            "\n",
            "[Training]\t loss (nn):14.2244 c_index: 0.4860, p-value: 9.697e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:83.0\n",
            "epcoh: 83\n",
            "\n",
            "[Training]\t loss (nn):14.2575 c_index: 0.4819, p-value: 3.509e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:84.0\n",
            "epcoh: 84\n",
            "\n",
            "[Training]\t loss (nn):14.2442 c_index: 0.4734, p-value: 3.632e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:85.0\n",
            "epcoh: 85\n",
            "\n",
            "[Training]\t loss (nn):14.3048 c_index: 0.5041, p-value: 5.063e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:86.0\n",
            "epcoh: 86\n",
            "\n",
            "[Training]\t loss (nn):14.2374 c_index: 0.5354, p-value: 2.712e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:87.0\n",
            "epcoh: 87\n",
            "\n",
            "[Training]\t loss (nn):14.2458 c_index: 0.4766, p-value: 3.356e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:88.0\n",
            "epcoh: 88\n",
            "\n",
            "[Training]\t loss (nn):14.2184 c_index: 0.5479, p-value: 4.273e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:89.0\n",
            "epcoh: 89\n",
            "\n",
            "[Training]\t loss (nn):14.2434 c_index: 0.4873, p-value: 7.369e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:90.0\n",
            "epcoh: 90\n",
            "\n",
            "[Training]\t loss (nn):14.2468 c_index: 0.5003, p-value: 4.677e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:91.0\n",
            "epcoh: 91\n",
            "\n",
            "[Training]\t loss (nn):14.2447 c_index: 0.4894, p-value: 8.188e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:92.0\n",
            "epcoh: 92\n",
            "\n",
            "[Training]\t loss (nn):14.2362 c_index: 0.5063, p-value: 9.027e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:93.0\n",
            "epcoh: 93\n",
            "\n",
            "[Training]\t loss (nn):14.2581 c_index: 0.5287, p-value: 2.915e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:94.0\n",
            "epcoh: 94\n",
            "\n",
            "[Training]\t loss (nn):14.2411 c_index: 0.4678, p-value: 3.356e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:95.0\n",
            "epcoh: 95\n",
            "\n",
            "[Training]\t loss (nn):14.2360 c_index: 0.5125, p-value: 8.962e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:96.0\n",
            "epcoh: 96\n",
            "\n",
            "[Training]\t loss (nn):14.2387 c_index: 0.4749, p-value: 4.403e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:97.0\n",
            "epcoh: 97\n",
            "\n",
            "[Training]\t loss (nn):14.2355 c_index: 0.5017, p-value: 5.615e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:98.0\n",
            "epcoh: 98\n",
            "\n",
            "[Training]\t loss (nn):14.2320 c_index: 0.4303, p-value: 7.980e-02\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n",
            "epoch:99.0\n",
            "epcoh: 99\n",
            "\n",
            "[Training]\t loss (nn):14.2225 c_index: 0.5057, p-value: 8.011e-01\n",
            "\n",
            "[Testing]\t loss (nn):2.6393 c_index: 0.5000, p-value: nan\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}